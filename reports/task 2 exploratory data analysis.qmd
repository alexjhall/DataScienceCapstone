---
title: "Task 2 - Exploratory Data Analysis"
subtitle: "John Hopkins University Data Science Specialisation"
author: "Alex Hall"
format: 
    html:
        fig-width: 12
        fig-height: 6
---

```{r, echo = FALSE, warning=FALSE, message=FALSE}
## Load packages
source(paste0(here::here(), "/R/packages.R"))


## Set options?


## Read data

# Single tokens (words)
word_df <- tar_read(preprocess_tokenise, store = here("_targets"))

# Bigrams
bigram_df <- tar_read(preprocess_tokenise_bigram, store = here("_targets"))

# Trigrams
trigram_df <- tar_read(preprocess_tokenise_trigram, store = here("_targets"))


```


# Introduction
## Background
This report represents 'Task 2' of the capstone project, as part of the John Hopkins University Data Science Specialisation.
The aim of the capstone project is to build a predictive text model that can predict the next word in a sentence. This model will be used for an R Shiny dashboard product and accompanying presentation.

Task 2 follows initial data loading and cleaning, described in more detail later on in this report, and aims to explore the data in more detail. Specific questions are outlined below.


## Report aims

### Tasks to accomplish

- Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora. 
- Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

### Questions to consider

- Some words are more frequent than others - what are the distributions of word frequencies? 
- What are the frequencies of 2-grams and 3-grams in the dataset? 
- How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? 
- How do you evaluate how many of the words come from foreign languages? 
- Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?


# Data
## Data sources
The raw data for this project was text in four languages: English, German, Finnish and Russian. For each language, there were three text files for text sourced from Twitter, News and Blogs. As an English speaker, I chose to only use the English language text files. 

## Data source summary
The following table shows a summary of the three source files. 

```{r, echo = FALSE}
tar_read(meta_summary, store = here("_targets")) %>%
    kable()

```


## Sampling
Given each source was a large dataset, initial exploration and modelling was conducted on a sample. If the amount of data turned out to be a problem, then more data could be sampled in the future.

In order to not bias the dataset, an equal number of lines were randomly sampled from each source, even though each had a very different number of lines. 10,000 was chosen as an arbitrary round number that made up nearly 10% of the Blogs dataset.


# Transformations
## Remove profanity
Course instructions in the first week suggested removing profanity from the data so that offensive words would not be modelled and therefore suggested as part of the prediction. 

This was done on the raw text using the sentimentr package's profanity() function to detect instances of profanity in a document and remove that document from the dataset. In practice, this removed <5% of documents so was deemed acceptable. The alternative would have been to remove single words which would have produced unnatural n-grams.

This was done before further pre-processing for two reasons. Firstly so that further steps would run more quickly with a slightly reduced dataset. Also, the profanity detection function depended on sentences, which could not be determined if punctuation was removed, which is what happens in later pre-processing steps.

## Further preprocessing
After documents containing profanity were removed, further text reprocessing was conducted to manipulate the data into a form suitable for text predictive models. This included removing digits, punctuation and stop words.


# Single word exploration


## Overall frequency

```{r}
## Count by word
count_word_df <- 
    word_df %>% count(word, sort = TRUE) %>%
    head(25) %>%
    mutate(word = reorder(word, n))

## plot
count_word_df %>%
    ggplot(aes(y = word, x = n)) +
    geom_col()


```



## Frequency by source

```{r}
## Count by word - grouped
count_word_df_grp <-
    word_df %>%
    dplyr::group_by(text_source) %>%
    count(word, sort = TRUE) %>%
    slice_head(n = 25)

## plot
count_word_df_grp %>%
    mutate(word = reorder_within(word, n, text_source)) %>%
    ggplot(aes(y = word, x = n)) +
    geom_col() +
    facet_wrap(vars(text_source), scales="free") + 
    scale_y_reordered() 
    


```

## Overall distribution

Overall histogram
Shows most values appear very few times


```{r}
## Count by word
count_word_df <- 
    word_df %>% count(word, sort = TRUE) %>%
    mutate(word = reorder(word, n))

## plot
count_word_df %>%
    ggplot(aes(x = n)) +
    # geom_histogram(binwidth = 50)
    geom_boxplot()


```


```{r, echo = FALSE}

## counts of single vals
single_word_count <-
    nrow(count_word_df[count_word_df$n ==1,])

## prop of count of single vals out of total.
single_word_prop <-
    single_word_count / nrow(count_word_df)


### Twitter
count_word_df_twitter <-
    count_word_df_grp %>%
    filter(text_source == "Twitter")

## counts of single vals
single_word_count_twitter <-
    nrow(count_word_df_twitter[count_word_df_twitter$n ==1,])

## prop of count of single vals out of total.
single_word_prop_twitter <-
    single_word_count_twitter / nrow(count_word_df_twitter)

           
### News
count_word_df_news <-
    count_word_df_grp %>%
    filter(text_source == "News")

## counts of single vals
single_word_count_news <-
    nrow(count_word_df_news[count_word_df_news$n ==1,])

## prop of count of single vals out of total.
single_word_prop_news <-
    single_word_count_news / nrow(count_word_df_news)



### Blogs
count_word_df_blogs <-
    count_word_df_grp %>%
    filter(text_source == "Blogs")


## counts of single vals
single_word_count_blogs <-
    nrow(count_word_df_blogs[count_word_df_blogs$n ==1,])

## prop of count of single vals out of total.
single_word_prop_blogs <-
    single_word_count_blogs / nrow(count_word_df_blogs)





```


## Distribution by source
Histogram by source
Twitter words...
suggests greater range of words. More words mentioned only once. back up with summary stats.




```{r}

## Count by word - grouped
count_word_df_grp <-
    word_df %>%
    dplyr::group_by(text_source) %>%
    count(word, sort = TRUE) %>%
    mutate(word = reorder(word, n))

## medians
median_word_df_grp <-
    count_word_df_grp %>%
    summarise(median = median(n))

## plot
count_word_df_grp %>%
    ggplot(aes(x = n, group = text_source, colour = text_source)) +
    # geom_histogram(binwidth = 50)
    geom_boxplot() +
    scale_x_continuous(limits = c(0, 100))



```


## Number of unique instances
How many with single mentions?
above.




# Bigram exploration

## Overall frequency

```{r}
## Count by bigram
count_bigram_df <- 
    bigram_df %>% count(bigram, sort = TRUE) %>%
    head(25) %>%
    mutate(bigram = reorder(bigram, n))

## plot
count_bigram_df %>%
    ggplot(aes(y = bigram, x = n)) +
    geom_col()


```


## Frequency by source


```{r}
## Count by word - grouped
count_bigram_df_grp <-
    bigram_df %>%
    dplyr::group_by(text_source) %>%
    count(bigram, sort = TRUE) %>%
    slice_head(n = 25)


## plot
count_bigram_df_grp %>%
    mutate(bigram = reorder_within(bigram, n, text_source)) %>%
    ggplot(aes(y = bigram, x = n)) +
    geom_col() +
    facet_wrap(vars(text_source), scales="free") +
    scale_y_reordered() 
    


```




# Trigram exploration

## Overall frequency

```{r}
## Count by trigram
count_trigram_df <- 
    trigram_df %>% count(trigram, sort = TRUE) %>%
    head(25) %>%
    mutate(trigram = reorder(trigram, n))

## plot
count_trigram_df %>%
    ggplot(aes(y = trigram, x = n)) +
    geom_col()


```


## Frequency by source


```{r}
## Count by word - grouped
count_trigram_df_grp <-
    trigram_df %>%
    dplyr::group_by(text_source) %>%
    count(trigram, sort = TRUE) %>%
    slice_head(n = 25)


## plot
count_trigram_df_grp %>%
    mutate(trigram = reorder_within(trigram, n, text_source)) %>%
    ggplot(aes(y = trigram, x = n)) +
    geom_col() +
    facet_wrap(vars(text_source), scales="free") +
    scale_y_reordered() 
    


```



# Unique words required for coverage

Look at cumsum with %

```{r}
## Count by word, with cumulative proportion and row number
count_word_df <- 
    word_df %>% count(word, sort = TRUE) %>%
    mutate(word = reorder(word, n)) %>%
    mutate(
        cum_prop = cumsum(n) / sum(n),
        no_words = row_number())

## work out min number of words
cov_words_50 <-
    count_word_df %>%
    filter(cum_prop >= 0.50) %>%
    select(no_words) %>%
    summarise(min_value = min(no_words)) %>%
    pull()


## work out min number of words
cov_words_90 <-
    count_word_df %>%
    filter(cum_prop >= 0.90) %>%
    select(no_words) %>%
    summarise(min_value = min(no_words)) %>%
    pull()


```

## 50% coverage





## 90% coverage






# Identify foreign languages





could also use...
English-language spellchecker (hunspell_check)


Link to repo: 
full code etc


